{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.database import registry\n",
    "from pyannote.audio.tasks import SpeakerDiarization\n",
    "from google.cloud import secretmanager\n",
    "from pyannote.audio import Model\n",
    "from pyannote.audio import Pipeline\n",
    "\n",
    "def access_secret_version(project_id, secret_id, version_id):\n",
    "    \"\"\"\n",
    "    Access the payload for the given secret version if one exists. The version\n",
    "    can be a version number as a string (e.g. \"5\") or an alias (e.g. \"latest\").\n",
    "    \"\"\"\n",
    "\n",
    "    client = secretmanager.SecretManagerServiceClient()\n",
    "\n",
    "    # Build the resource name of the secret version.\n",
    "    name = f\"projects/{project_id}/secrets/{secret_id}/versions/{version_id}\"\n",
    "\n",
    "    # Access the secret version.\n",
    "    response = client.access_secret_version(name=name)\n",
    "\n",
    "    # Return the decoded payload.\n",
    "    return response.payload.data.decode('UTF-8')\n",
    "\n",
    "HF_KEY = access_secret_version(1076126751560, \"huggingface-api-key\", \"latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m registry\u001b[39m.\u001b[39mload_database(\u001b[39m\"\u001b[39m\u001b[39m/Users/StephenWitkowski/Projects/broadcastify-transcription/data/broadcastify_database.yaml\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m protocol \u001b[39m=\u001b[39m registry\u001b[39m.\u001b[39;49mget_protocol(\u001b[39m'\u001b[39;49m\u001b[39mMyDatabase.SpeakerDiarization\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/micromamba/envs/broadcastify/lib/python3.11/site-packages/pyannote/database/registry.py:324\u001b[0m, in \u001b[0;36mRegistry.get_protocol\u001b[0;34m(self, name, preprocessors)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_protocol\u001b[39m(\u001b[39mself\u001b[39m, name, preprocessors: Optional[Preprocessors] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Protocol:\n\u001b[1;32m    306\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get protocol by full name\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \n\u001b[1;32m    308\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39m        Protocol instance\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 324\u001b[0m     database_name, task_name, protocol_name \u001b[39m=\u001b[39m name\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    325\u001b[0m     database \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_database(database_name)\n\u001b[1;32m    326\u001b[0m     protocol \u001b[39m=\u001b[39m database\u001b[39m.\u001b[39mget_protocol(\n\u001b[1;32m    327\u001b[0m         task_name, protocol_name, preprocessors\u001b[39m=\u001b[39mpreprocessors\n\u001b[1;32m    328\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "registry.load_database(\"/Users/StephenWitkowski/Projects/broadcastify-transcription/data/broadcastify_database.yaml\")\n",
    "protocol = registry.get_protocol('MyDatabase.SpeakerDiarization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchvision is not available - cannot save figures\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.0.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../../../../.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.0.1. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pretrained = pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization@2.1\", use_auth_token=HF_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyannote.database.registry.MyDatabase at 0x156db9bd0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "registry.get_database(\"MyDatabase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = registry.get_database('MyDatabase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol = registry.get_protocol('MyDatabase.SpeakerDiarization.MyProtocol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYANNOTE_DATABASE_CONFIG\"] = \"/Users/StephenWitkowski/Projects/broadcastify-transcription/data/broadcastify_database.yaml\"\n",
    "from pyannote.database import get_protocol, FileFinder\n",
    "dataset = get_protocol(\"MyDatabase.SpeakerDiarization.MyProtocol\", {\"audio\": FileFinder()})\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.0.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../../../../.cache/torch/pyannote/models--pyannote--segmentation/snapshots/7d5cf7bca4dcac7f943eb834bec0906a90da8c97/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.0.1. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "from pyannote.audio import Model\n",
    "model = Model.from_pretrained(\"pyannote/segmentation\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mSegmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprotocol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpyannote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspeaker_diarization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpeakerDiarizationProtocol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mduration\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_speakers_per_chunk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_speakers_per_frame\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mweigh_by_cardinality\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mwarm_up\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbalance\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mweight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_workers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0maugmentation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch_audiomentations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms_interface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseWaveformTransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mvad_loss\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bce'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mse'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmetric\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorchmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_num_speakers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mloss\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bce'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mse'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Speaker diarization\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "protocol : SpeakerDiarizationProtocol\n",
      "    pyannote.database protocol\n",
      "duration : float, optional\n",
      "    Chunks duration. Defaults to 2s.\n",
      "max_speakers_per_chunk : int, optional\n",
      "    Maximum number of speakers per chunk (must be at least 2).\n",
      "    Defaults to estimating it from the training set.\n",
      "max_speakers_per_frame : int, optional\n",
      "    Maximum number of (overlapping) speakers per frame.\n",
      "    Setting this value to 1 or more enables `powerset multi-class` training.\n",
      "    Default behavior is to use `multi-label` training.\n",
      "weigh_by_cardinality: bool, optional\n",
      "    Weigh each powerset classes by the size of the corresponding speaker set.\n",
      "    In other words, {0, 1} powerset class weight is 2x bigger than that of {0}\n",
      "    or {1} powerset classes. Note that empty (non-speech) powerset class is\n",
      "    assigned the same weight as mono-speaker classes. Defaults to False (i.e. use\n",
      "    same weight for every class). Has no effect with `multi-label` training.\n",
      "warm_up : float or (float, float), optional\n",
      "    Use that many seconds on the left- and rightmost parts of each chunk\n",
      "    to warm up the model. While the model does process those left- and right-most\n",
      "    parts, only the remaining central part of each chunk is used for computing the\n",
      "    loss during training, and for aggregating scores during inference.\n",
      "    Defaults to 0. (i.e. no warm-up).\n",
      "balance: str, optional\n",
      "    When provided, training samples are sampled uniformly with respect to that key.\n",
      "    For instance, setting `balance` to \"database\" will make sure that each database\n",
      "    will be equally represented in the training samples.\n",
      "weight: str, optional\n",
      "    When provided, use this key as frame-wise weight in loss function.\n",
      "batch_size : int, optional\n",
      "    Number of training samples per batch. Defaults to 32.\n",
      "num_workers : int, optional\n",
      "    Number of workers used for generating training samples.\n",
      "    Defaults to multiprocessing.cpu_count() // 2.\n",
      "pin_memory : bool, optional\n",
      "    If True, data loaders will copy tensors into CUDA pinned\n",
      "    memory before returning them. See pytorch documentation\n",
      "    for more details. Defaults to False.\n",
      "augmentation : BaseWaveformTransform, optional\n",
      "    torch_audiomentations waveform transform, used by dataloader\n",
      "    during training.\n",
      "vad_loss : {\"bce\", \"mse\"}, optional\n",
      "    Add voice activity detection loss.\n",
      "    Cannot be used in conjunction with `max_speakers_per_frame`.\n",
      "metric : optional\n",
      "    Validation metric(s). Can be anything supported by torchmetrics.MetricCollection.\n",
      "    Defaults to AUROC (area under the ROC curve).\n",
      "\n",
      "References\n",
      "----------\n",
      "Hervé Bredin and Antoine Laurent\n",
      "\"End-To-End Speaker Segmentation for Overlap-Aware Resegmentation.\"\n",
      "Proc. Interspeech 2021\n",
      "\n",
      "Zhihao Du, Shiliang Zhang, Siqi Zheng, and Zhijie Yan\n",
      "\"Speaker Embedding-aware Neural Diarization: an Efficient Framework for Overlapping\n",
      "Speech Diarization in Meeting Scenarios\"\n",
      "https://arxiv.org/abs/2203.09767\n",
      "\u001b[0;31mFile:\u001b[0m           ~/micromamba/envs/broadcastify/lib/python3.11/site-packages/pyannote/audio/tasks/segmentation/speaker_diarization.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "Segmentation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio.tasks import Segmentation\n",
    "task = Segmentation(\n",
    "    dataset, \n",
    "    duration=2, \n",
    "    max_num_speakers=10, \n",
    "    batch_size=32,\n",
    "    num_workers=2, \n",
    "    loss=\"bce\", \n",
    "    vad_loss=\"bce\")\n",
    "model.task = task\n",
    "model.setup(stage=\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0maccelerator\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAccelerator\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstrategy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStrategy\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdevices\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_nodes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprecision\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'16-mixed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bf16-mixed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'32-true'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'64-true'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'64'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'32'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'16'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bf16'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'32-true'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlogger\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloggers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloggers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCallback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfast_dev_run\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_epochs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmin_epochs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmin_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_time\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlimit_train_batches\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlimit_val_batches\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlimit_test_batches\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlimit_predict_batches\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moverfit_batches\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mval_check_interval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcheck_val_every_n_epoch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_sanity_val_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlog_every_n_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0menable_checkpointing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0menable_progress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0menable_model_summary\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0maccumulate_grad_batches\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mgradient_clip_val\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mgradient_clip_algorithm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdeterministic\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'warn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbenchmark\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minference_mode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0muse_distributed_sampler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprofiler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofilers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProfiler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdetect_anomaly\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbarebones\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mplugins\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrecisionPlugin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlightning_fabric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_environment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClusterEnvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlightning_fabric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCheckpointIO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_sync\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerSync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrecisionPlugin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlightning_fabric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_environment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClusterEnvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlightning_fabric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCheckpointIO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_sync\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerSync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msync_batchnorm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreload_dataloaders_every_n_epochs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdefault_root_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m      <no docstring>\n",
      "\u001b[0;31mInit docstring:\u001b[0m\n",
      "Customize every aspect of training via flags.\n",
      "\n",
      "Args:\n",
      "    accelerator: Supports passing different accelerator types (\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"mps\", \"auto\")\n",
      "        as well as custom accelerator instances.\n",
      "\n",
      "    strategy: Supports different training strategies with aliases as well custom strategies.\n",
      "        Default: ``\"auto\"``.\n",
      "\n",
      "    devices: The devices to use. Can be set to a positive number (int or str), a sequence of device indices\n",
      "        (list or str), the value ``-1`` to indicate all available devices should be used, or ``\"auto\"`` for\n",
      "        automatic selection based on the chosen accelerator. Default: ``\"auto\"``.\n",
      "\n",
      "    num_nodes: Number of GPU nodes for distributed training.\n",
      "        Default: ``1``.\n",
      "\n",
      "    precision: Double precision (64, '64' or '64-true'), full precision (32, '32' or '32-true'),\n",
      "        16bit mixed precision (16, '16', '16-mixed') or bfloat16 mixed precision ('bf16', 'bf16-mixed').\n",
      "        Can be used on CPU, GPU, TPUs, HPUs or IPUs.\n",
      "        Default: ``'32-true'``.\n",
      "\n",
      "    logger: Logger (or iterable collection of loggers) for experiment tracking. A ``True`` value uses\n",
      "        the default ``TensorBoardLogger`` if it is installed, otherwise ``CSVLogger``.\n",
      "        ``False`` will disable logging. If multiple loggers are provided, local files\n",
      "        (checkpoints, profiler traces, etc.) are saved in the ``log_dir`` of he first logger.\n",
      "        Default: ``True``.\n",
      "\n",
      "    callbacks: Add a callback or list of callbacks.\n",
      "        Default: ``None``.\n",
      "\n",
      "    fast_dev_run: Runs n if set to ``n`` (int) else 1 if set to ``True`` batch(es)\n",
      "        of train, val and test to find any bugs (ie: a sort of unit test).\n",
      "        Default: ``False``.\n",
      "\n",
      "    max_epochs: Stop training once this number of epochs is reached. Disabled by default (None).\n",
      "        If both max_epochs and max_steps are not specified, defaults to ``max_epochs = 1000``.\n",
      "        To enable infinite training, set ``max_epochs = -1``.\n",
      "\n",
      "    min_epochs: Force training for at least these many epochs. Disabled by default (None).\n",
      "\n",
      "    max_steps: Stop training after this number of steps. Disabled by default (-1). If ``max_steps = -1``\n",
      "        and ``max_epochs = None``, will default to ``max_epochs = 1000``. To enable infinite training, set\n",
      "        ``max_epochs`` to ``-1``.\n",
      "\n",
      "    min_steps: Force training for at least these number of steps. Disabled by default (``None``).\n",
      "\n",
      "    max_time: Stop training after this amount of time has passed. Disabled by default (``None``).\n",
      "        The time duration can be specified in the format DD:HH:MM:SS (days, hours, minutes seconds), as a\n",
      "        :class:`datetime.timedelta`, or a dictionary with keys that will be passed to\n",
      "        :class:`datetime.timedelta`.\n",
      "\n",
      "    limit_train_batches: How much of training dataset to check (float = fraction, int = num_batches).\n",
      "        Default: ``1.0``.\n",
      "\n",
      "    limit_val_batches: How much of validation dataset to check (float = fraction, int = num_batches).\n",
      "        Default: ``1.0``.\n",
      "\n",
      "    limit_test_batches: How much of test dataset to check (float = fraction, int = num_batches).\n",
      "        Default: ``1.0``.\n",
      "\n",
      "    limit_predict_batches: How much of prediction dataset to check (float = fraction, int = num_batches).\n",
      "        Default: ``1.0``.\n",
      "\n",
      "    overfit_batches: Overfit a fraction of training/validation data (float) or a set number of batches (int).\n",
      "        Default: ``0.0``.\n",
      "\n",
      "    val_check_interval: How often to check the validation set. Pass a ``float`` in the range [0.0, 1.0] to check\n",
      "        after a fraction of the training epoch. Pass an ``int`` to check after a fixed number of training\n",
      "        batches. An ``int`` value can only be higher than the number of training batches when\n",
      "        ``check_val_every_n_epoch=None``, which validates after every ``N`` training batches\n",
      "        across epochs or during iteration-based training.\n",
      "        Default: ``1.0``.\n",
      "\n",
      "    check_val_every_n_epoch: Perform a validation loop every after every `N` training epochs. If ``None``,\n",
      "        validation will be done solely based on the number of training batches, requiring ``val_check_interval``\n",
      "        to be an integer value.\n",
      "        Default: ``1``.\n",
      "\n",
      "    num_sanity_val_steps: Sanity check runs n validation batches before starting the training routine.\n",
      "        Set it to `-1` to run all batches in all validation dataloaders.\n",
      "        Default: ``2``.\n",
      "\n",
      "    log_every_n_steps: How often to log within steps.\n",
      "        Default: ``50``.\n",
      "\n",
      "    enable_checkpointing: If ``True``, enable checkpointing.\n",
      "        It will configure a default ModelCheckpoint callback if there is no user-defined ModelCheckpoint in\n",
      "        :paramref:`~pytorch_lightning.trainer.trainer.Trainer.callbacks`.\n",
      "        Default: ``True``.\n",
      "\n",
      "    enable_progress_bar: Whether to enable to progress bar by default.\n",
      "        Default: ``True``.\n",
      "\n",
      "    enable_model_summary: Whether to enable model summarization by default.\n",
      "        Default: ``True``.\n",
      "\n",
      "    accumulate_grad_batches: Accumulates gradients over k batches before stepping the optimizer.\n",
      "        Default: 1.\n",
      "\n",
      "    gradient_clip_val: The value at which to clip gradients. Passing ``gradient_clip_val=None`` disables\n",
      "        gradient clipping. If using Automatic Mixed Precision (AMP), the gradients will be unscaled before.\n",
      "        Default: ``None``.\n",
      "\n",
      "    gradient_clip_algorithm: The gradient clipping algorithm to use. Pass ``gradient_clip_algorithm=\"value\"``\n",
      "        to clip by value, and ``gradient_clip_algorithm=\"norm\"`` to clip by norm. By default it will\n",
      "        be set to ``\"norm\"``.\n",
      "\n",
      "    deterministic: If ``True``, sets whether PyTorch operations must use deterministic algorithms.\n",
      "        Set to ``\"warn\"`` to use deterministic algorithms whenever possible, throwing warnings on operations\n",
      "        that don't support deterministic mode (requires PyTorch 1.11+). If not set, defaults to ``False``.\n",
      "        Default: ``None``.\n",
      "\n",
      "    benchmark: The value (``True`` or ``False``) to set ``torch.backends.cudnn.benchmark`` to.\n",
      "        The value for ``torch.backends.cudnn.benchmark`` set in the current session will be used\n",
      "        (``False`` if not manually set). If :paramref:`~pytorch_lightning.trainer.trainer.Trainer.deterministic`\n",
      "        is set to ``True``, this will default to ``False``. Override to manually set a different value.\n",
      "        Default: ``None``.\n",
      "\n",
      "    inference_mode: Whether to use :func:`torch.inference_mode` or :func:`torch.no_grad` during\n",
      "        evaluation (``validate``/``test``/``predict``).\n",
      "\n",
      "    use_distributed_sampler: Whether to wrap the DataLoader's sampler with\n",
      "        :class:`torch.utils.data.DistributedSampler`. If not specified this is toggled automatically for\n",
      "        strategies that require it. By default, it will add ``shuffle=True`` for the train sampler and\n",
      "        ``shuffle=False`` for validation/test/predict samplers. If you want to disable this logic, you can pass\n",
      "        ``False`` and add your own distributed sampler in the dataloader hooks. If ``True`` and a distributed\n",
      "        sampler was already added, Lightning will not replace the existing one. For iterable-style datasets,\n",
      "        we don't do this automatically.\n",
      "\n",
      "    profiler: To profile individual steps during training and assist in identifying bottlenecks.\n",
      "        Default: ``None``.\n",
      "\n",
      "    detect_anomaly: Enable anomaly detection for the autograd engine.\n",
      "        Default: ``False``.\n",
      "\n",
      "    barebones: Whether to run in \"barebones mode\", where all features that may impact raw speed are\n",
      "        disabled. This is meant for analyzing the Trainer overhead and is discouraged during regular training\n",
      "        runs. The following features are deactivated:\n",
      "        :paramref:`~pytorch_lightning.trainer.trainer.Trainer.enable_checkpointing`,\n",
      "        :paramref:`~pytorch_lightning.trainer.trainer.Trainer.logger`,\n",
      "        :paramref:`~pytorch_lightning.trainer.trainer.Trainer.enable_progress_bar`,\n",
      "        :paramref:`~pytorch_lightning.trainer.trainer.Trainer.log_every_n_steps`,\n",
      "        :paramref:`~pytorch_lightning.trainer.trainer.Trainer.enable_model_summary`,\n",
      "        :paramref:`~pytorch_lightning.trainer.trainer.Trainer.num_sanity_val_steps`,\n",
      "        :paramref:`~pytorch_lightning.trainer.trainer.Trainer.fast_dev_run`,\n",
      "        :paramref:`~pytorch_lightning.trainer.trainer.Trainer.detect_anomaly`,\n",
      "        :paramref:`~pytorch_lightning.trainer.trainer.Trainer.profiler`,\n",
      "        :meth:`~pytorch_lightning.core.module.LightningModule.log`,\n",
      "        :meth:`~pytorch_lightning.core.module.LightningModule.log_dict`.\n",
      "    plugins: Plugins allow modification of core behavior like ddp and amp, and enable custom lightning plugins.\n",
      "        Default: ``None``.\n",
      "\n",
      "    sync_batchnorm: Synchronize batch norm layers between process groups/whole world.\n",
      "        Default: ``False``.\n",
      "\n",
      "    reload_dataloaders_every_n_epochs: Set to a non-negative integer to reload dataloaders every n epochs.\n",
      "        Default: ``0``.\n",
      "\n",
      "    default_root_dir: Default path for logs and weights when no logger/ckpt_callback passed.\n",
      "        Default: ``os.getcwd()``.\n",
      "        Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/'\n",
      "\n",
      "Raises:\n",
      "    TypeError:\n",
      "        If ``gradient_clip_val`` is not an int or float.\n",
      "\n",
      "    MisconfigurationException:\n",
      "        If ``gradient_clip_algorithm`` is invalid.\n",
      "        If ``track_grad_norm`` is not a positive number or inf.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/micromamba/envs/broadcastify/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "?Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name              </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">      In sizes </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">                                   Out sizes </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ sincnet           │ SincNet          │ 42.6 K │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> [1, 1, 32000] </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                                [1, 60, 115] </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ lstm              │ LSTM             │  1.4 M │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">  [1, 115, 60] </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> [[1, 115, 256], [[8, 1, 128], [8, 1, 128]]] </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ linear            │ ModuleList       │ 49.4 K │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">             ? </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                                           ? </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ classifier        │ Linear           │  1.3 K │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> [1, 115, 128] </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                                [1, 115, 10] </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ activation        │ Sigmoid          │      0 │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">  [1, 115, 10] </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                                [1, 115, 10] </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ validation_metric │ MetricCollection │      0 │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">             ? </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                                           ? </span>│\n",
       "└───┴───────────────────┴──────────────────┴────────┴───────────────┴─────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName             \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35m     In sizes\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35m                                  Out sizes\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ sincnet           │ SincNet          │ 42.6 K │\u001b[37m \u001b[0m\u001b[37m[1, 1, 32000]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m                               [1, 60, 115]\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ lstm              │ LSTM             │  1.4 M │\u001b[37m \u001b[0m\u001b[37m [1, 115, 60]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m[[1, 115, 256], [[8, 1, 128], [8, 1, 128]]]\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ linear            │ ModuleList       │ 49.4 K │\u001b[37m \u001b[0m\u001b[37m            ?\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m                                          ?\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ classifier        │ Linear           │  1.3 K │\u001b[37m \u001b[0m\u001b[37m[1, 115, 128]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m                               [1, 115, 10]\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ activation        │ Sigmoid          │      0 │\u001b[37m \u001b[0m\u001b[37m [1, 115, 10]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m                               [1, 115, 10]\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ validation_metric │ MetricCollection │      0 │\u001b[37m \u001b[0m\u001b[37m            ?\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m                                          ?\u001b[0m\u001b[37m \u001b[0m│\n",
       "└───┴───────────────────┴──────────────────┴────────┴───────────────┴─────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 1.5 M                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 1.5 M                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 5                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 1.5 M                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 1.5 M                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 5                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ceabde3e834d67bfced7a1c6d33656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "Could not find the `LightningModule` attribute for the `torchmetrics.Metric` logged. You can fix this by calling `self.log(DiarizationErrorRate, ..., metric_attribute=name)` where `name` is one of ['validation_metric.DiarizationErrorRate', 'validation_metric.DiarizationErrorRate/Confusion', 'validation_metric.DiarizationErrorRate/FalseAlarm', 'validation_metric.DiarizationErrorRate/Miss', 'validation_metric.DiarizationErrorRate/Threshold']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 46\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m \u001b[39mimport\u001b[39;00m Trainer\n\u001b[1;32m     42\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(accelerator\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m     43\u001b[0m                   callbacks\u001b[39m=\u001b[39mcallbacks, \n\u001b[1;32m     44\u001b[0m                   max_epochs\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m,\n\u001b[1;32m     45\u001b[0m                   gradient_clip_val\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model)\n",
      "File \u001b[0;32m~/micromamba/envs/broadcastify/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:529\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    527\u001b[0m model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    528\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 529\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    530\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    531\u001b[0m )\n",
      "File \u001b[0;32m~/micromamba/envs/broadcastify/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:42\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     44\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     45\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/micromamba/envs/broadcastify/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:568\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[1;32m    559\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[1;32m    560\u001b[0m )\n\u001b[1;32m    562\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    563\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    564\u001b[0m     ckpt_path,\n\u001b[1;32m    565\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    566\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    567\u001b[0m )\n\u001b[0;32m--> 568\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    570\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    571\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/broadcastify/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:973\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    970\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 973\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    975\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    977\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    978\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/broadcastify/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1014\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m   1013\u001b[0m     \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1014\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   1015\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1016\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/micromamba/envs/broadcastify/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1043\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1040\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1042\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1045\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1047\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/broadcastify/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:177\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[1;32m    176\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 177\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/micromamba/envs/broadcastify/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:115\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m     previous_dataloader_idx \u001b[39m=\u001b[39m dataloader_idx\n\u001b[1;32m    114\u001b[0m     \u001b[39m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(batch, batch_idx, dataloader_idx)\n\u001b[1;32m    116\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[39m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/broadcastify/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:375\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    374\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 375\u001b[0m output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, hook_name, \u001b[39m*\u001b[39;49mstep_kwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    377\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    379\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mon_test_batch_end\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mon_validation_batch_end\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/micromamba/envs/broadcastify/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:291\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 291\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    293\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    294\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/micromamba/envs/broadcastify/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:379\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mval_step_context():\n\u001b[1;32m    378\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, ValidationStep)\n\u001b[0;32m--> 379\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/micromamba/envs/broadcastify/lib/python3.11/site-packages/pyannote/audio/core/model.py:367\u001b[0m, in \u001b[0;36mModel.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidation_step\u001b[39m(\u001b[39mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m--> 367\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtask\u001b[39m.\u001b[39;49mvalidation_step(batch, batch_idx)\n",
      "File \u001b[0;32m~/micromamba/envs/broadcastify/lib/python3.11/site-packages/pyannote/audio/tasks/segmentation/speaker_diarization.py:763\u001b[0m, in \u001b[0;36mSpeakerDiarization.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    754\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mvalidation_metric(\n\u001b[1;32m    755\u001b[0m         torch\u001b[39m.\u001b[39mtranspose(\n\u001b[1;32m    756\u001b[0m             prediction[:, warm_up_left : num_frames \u001b[39m-\u001b[39m warm_up_right], \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    760\u001b[0m         ),\n\u001b[1;32m    761\u001b[0m     )\n\u001b[0;32m--> 763\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mlog_dict(\n\u001b[1;32m    764\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mvalidation_metric,\n\u001b[1;32m    765\u001b[0m     on_step\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    766\u001b[0m     on_epoch\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    767\u001b[0m     prog_bar\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    768\u001b[0m     logger\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    769\u001b[0m )\n\u001b[1;32m    771\u001b[0m \u001b[39m# log first batch visualization every 2^n epochs.\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    773\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mcurrent_epoch \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    774\u001b[0m     \u001b[39mor\u001b[39;00m math\u001b[39m.\u001b[39mlog2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mcurrent_epoch) \u001b[39m%\u001b[39m \u001b[39m1\u001b[39m \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    775\u001b[0m     \u001b[39mor\u001b[39;00m batch_idx \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    776\u001b[0m ):\n",
      "File \u001b[0;32m~/micromamba/envs/broadcastify/lib/python3.11/site-packages/pytorch_lightning/core/module.py:567\u001b[0m, in \u001b[0;36mLightningModule.log_dict\u001b[0;34m(self, dictionary, prog_bar, logger, on_step, on_epoch, reduce_fx, enable_graph, sync_dist, sync_dist_group, add_dataloader_idx, batch_size, rank_zero_only)\u001b[0m\n\u001b[1;32m    564\u001b[0m         kwargs[\u001b[39m\"\u001b[39m\u001b[39mcopy_state\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m dictionary\u001b[39m.\u001b[39mitems(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 567\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog(\n\u001b[1;32m    568\u001b[0m         name\u001b[39m=\u001b[39;49mk,\n\u001b[1;32m    569\u001b[0m         value\u001b[39m=\u001b[39;49mv,\n\u001b[1;32m    570\u001b[0m         prog_bar\u001b[39m=\u001b[39;49mprog_bar,\n\u001b[1;32m    571\u001b[0m         logger\u001b[39m=\u001b[39;49mlogger,\n\u001b[1;32m    572\u001b[0m         on_step\u001b[39m=\u001b[39;49mon_step,\n\u001b[1;32m    573\u001b[0m         on_epoch\u001b[39m=\u001b[39;49mon_epoch,\n\u001b[1;32m    574\u001b[0m         reduce_fx\u001b[39m=\u001b[39;49mreduce_fx,\n\u001b[1;32m    575\u001b[0m         enable_graph\u001b[39m=\u001b[39;49menable_graph,\n\u001b[1;32m    576\u001b[0m         sync_dist\u001b[39m=\u001b[39;49msync_dist,\n\u001b[1;32m    577\u001b[0m         sync_dist_group\u001b[39m=\u001b[39;49msync_dist_group,\n\u001b[1;32m    578\u001b[0m         add_dataloader_idx\u001b[39m=\u001b[39;49madd_dataloader_idx,\n\u001b[1;32m    579\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    580\u001b[0m         rank_zero_only\u001b[39m=\u001b[39;49mrank_zero_only,\n\u001b[1;32m    581\u001b[0m     )\n\u001b[1;32m    582\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/broadcastify/lib/python3.11/site-packages/pytorch_lightning/core/module.py:462\u001b[0m, in \u001b[0;36mLightningModule.log\u001b[0;34m(self, name, value, prog_bar, logger, on_step, on_epoch, reduce_fx, enable_graph, sync_dist, sync_dist_group, add_dataloader_idx, batch_size, metric_attribute, rank_zero_only)\u001b[0m\n\u001b[1;32m    460\u001b[0m     metric_attribute \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metric_attributes\u001b[39m.\u001b[39mget(\u001b[39mid\u001b[39m(value), \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    461\u001b[0m     \u001b[39mif\u001b[39;00m metric_attribute \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 462\u001b[0m         \u001b[39mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m    463\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCould not find the `LightningModule` attribute for the `torchmetrics.Metric` logged.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    464\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m You can fix this by calling `self.log(\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m, ..., metric_attribute=name)` where `name` is one\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    465\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metric_attributes\u001b[39m.\u001b[39mvalues())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    466\u001b[0m         )\n\u001b[1;32m    468\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    469\u001b[0m     trainer\u001b[39m.\u001b[39mtraining\n\u001b[1;32m    470\u001b[0m     \u001b[39mand\u001b[39;00m is_param_in_hook_signature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step, \u001b[39m\"\u001b[39m\u001b[39mdataloader_iter\u001b[39m\u001b[39m\"\u001b[39m, explicit\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    471\u001b[0m     \u001b[39mand\u001b[39;00m batch_size \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    472\u001b[0m ):\n\u001b[1;32m    473\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m    474\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWith `def training_step(self, dataloader_iter)`, `self.log(..., batch_size=...)` should be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    475\u001b[0m     )\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: Could not find the `LightningModule` attribute for the `torchmetrics.Metric` logged. You can fix this by calling `self.log(DiarizationErrorRate, ..., metric_attribute=name)` where `name` is one of ['validation_metric.DiarizationErrorRate', 'validation_metric.DiarizationErrorRate/Confusion', 'validation_metric.DiarizationErrorRate/FalseAlarm', 'validation_metric.DiarizationErrorRate/Miss', 'validation_metric.DiarizationErrorRate/Threshold']"
     ]
    }
   ],
   "source": [
    "# this takes approximately 15min to run on Google Colab GPU\n",
    "from types import MethodType\n",
    "from torch.optim import Adam\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    RichProgressBar,\n",
    ")\n",
    "\n",
    "# we use Adam optimizer with 1e-4 learning rate\n",
    "def configure_optimizers(self):\n",
    "    return Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "model.configure_optimizers = MethodType(configure_optimizers, model)\n",
    "\n",
    "# we monitor diarization error rate on the validation set\n",
    "# and use to keep the best checkpoint and stop early\n",
    "monitor, direction = task.val_monitor\n",
    "checkpoint = ModelCheckpoint(\n",
    "    monitor=monitor,\n",
    "    mode=direction,\n",
    "    save_top_k=1,\n",
    "    every_n_epochs=1,\n",
    "    save_last=False,\n",
    "    save_weights_only=False,\n",
    "    filename=\"{epoch}\",\n",
    "    verbose=False,\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=monitor,\n",
    "    mode=direction,\n",
    "    min_delta=0.0,\n",
    "    patience=10,\n",
    "    strict=True,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "callbacks = [RichProgressBar(), checkpoint, early_stopping]\n",
    "\n",
    "# we train for at most 20 epochs (might be shorter in case of early stopping)\n",
    "from pytorch_lightning import Trainer\n",
    "trainer = Trainer(accelerator=\"cpu\", \n",
    "                  callbacks=callbacks, \n",
    "                  max_epochs=20,\n",
    "                  gradient_clip_val=0.5)\n",
    "trainer.fit(model)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Protocol.train() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyannote\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maudio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtasks\u001b[39;00m \u001b[39mimport\u001b[39;00m SpeakerDiarization\n\u001b[0;32m----> 2\u001b[0m seg_task \u001b[39m=\u001b[39m SpeakerDiarization(protocol, duration\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
      "File \u001b[0;32m~/micromamba/envs/broadcastify/lib/python3.11/site-packages/pyannote/audio/tasks/segmentation/speaker_diarization.py:146\u001b[0m, in \u001b[0;36mSpeakerDiarization.__init__\u001b[0;34m(self, protocol, duration, max_speakers_per_chunk, max_speakers_per_frame, weigh_by_cardinality, warm_up, balance, weight, batch_size, num_workers, pin_memory, augmentation, vad_loss, metric, max_num_speakers, loss)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    128\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    129\u001b[0m     protocol: SpeakerDiarizationProtocol,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m     loss: Literal[\u001b[39m\"\u001b[39m\u001b[39mbce\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,  \u001b[39m# deprecated\u001b[39;00m\n\u001b[1;32m    145\u001b[0m ):\n\u001b[0;32m--> 146\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    147\u001b[0m         protocol,\n\u001b[1;32m    148\u001b[0m         duration\u001b[39m=\u001b[39;49mduration,\n\u001b[1;32m    149\u001b[0m         warm_up\u001b[39m=\u001b[39;49mwarm_up,\n\u001b[1;32m    150\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    151\u001b[0m         num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[1;32m    152\u001b[0m         pin_memory\u001b[39m=\u001b[39;49mpin_memory,\n\u001b[1;32m    153\u001b[0m         augmentation\u001b[39m=\u001b[39;49maugmentation,\n\u001b[1;32m    154\u001b[0m         metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[1;32m    155\u001b[0m     )\n\u001b[1;32m    157\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(protocol, SpeakerDiarizationProtocol):\n\u001b[1;32m    158\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    159\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mSpeakerDiarization task requires a SpeakerDiarizationProtocol.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         )\n",
      "File \u001b[0;32m~/micromamba/envs/broadcastify/lib/python3.11/site-packages/pyannote/audio/core/task.py:221\u001b[0m, in \u001b[0;36mTask.__init__\u001b[0;34m(self, protocol, duration, min_duration, warm_up, batch_size, num_workers, pin_memory, augmentation, metric)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m    220\u001b[0m \u001b[39m# dataset\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprotocol, checks \u001b[39m=\u001b[39m check_protocol(protocol)\n\u001b[1;32m    222\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_validation \u001b[39m=\u001b[39m checks[\u001b[39m\"\u001b[39m\u001b[39mhas_validation\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    223\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_scope \u001b[39m=\u001b[39m checks[\u001b[39m\"\u001b[39m\u001b[39mhas_scope\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/micromamba/envs/broadcastify/lib/python3.11/site-packages/pyannote/audio/utils/protocol.py:56\u001b[0m, in \u001b[0;36mcheck_protocol\u001b[0;34m(protocol)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39m# does protocol define a training set?\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     file \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(protocol\u001b[39m.\u001b[39;49mtrain())\n\u001b[1;32m     57\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mAttributeError\u001b[39;00m, \u001b[39mNotImplementedError\u001b[39;00m):\n\u001b[1;32m     58\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProtocol \u001b[39m\u001b[39m{\u001b[39;00mprotocol\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m does not define a training set.\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Protocol.train() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "from pyannote.audio.tasks import SpeakerDiarization\n",
    "seg_task = SpeakerDiarization(protocol, duration=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# /path/to/train.lst\n",
      "filename1\n",
      "filename2\n"
     ]
    }
   ],
   "source": [
    "protocol = registry.get_protocol('MyDatabase.Protocol.MyProtocol')\n",
    "for resource in protocol.train():\n",
    "    print(resource[\"uri\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPEAKER gs://purple-label-studio-data/broadcastify/input/input_chunk_108.wav 1 1.53 3.38 <NA> <NA> A <NA> <NA>\n",
      "SPEAKER gs://purple-label-studio-data/broadcastify/input/input_chunk_108.wav 1 10.34 1.47 <NA> <NA> B <NA> <NA>\n",
      "SPEAKER gs://purple-label-studio-data/broadcastify/input/input_chunk_108.wav 1 39.88 1.98 <NA> <NA> C <NA> <NA>\n",
      "SPEAKER gs://purple-label-studio-data/broadcastify/input/input_chunk_108.wav 1 47.98 1.72 <NA> <NA> D <NA> <NA>\n",
      "SPEAKER gs://purple-label-studio-data/broadcastify/input/input_chunk_108.wav 1 50.41 3.19 <NA> <NA> E <NA> <NA>\n",
      "SPEAKER gs://purple-label-studio-data/broadcastify/input/input_chunk_108.wav 1 56.98 4.02 <NA> <NA> F <NA> <NA>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Your given JSON data\n",
    "data_json = \"\"\"{\"id\": 10, \"created_username\": \" stephen.witkowski@protonmail.com, 1\", \"created_ago\": \"0\\u00a0minutes\", \"completed_by\": {\"id\": 1, \"first_name\": \"\", \"last_name\": \"\", \"email\": \"stephen.witkowski@protonmail.com\"}, \"task\": {\"id\": 10, \"data\": {\"audio\": \"gs://purple-label-studio-data/broadcastify/input/input_chunk_108.wav\"}, \"meta\": {}, \"created_at\": \"2023-08-04T16:20:11.293823Z\", \"updated_at\": \"2023-08-04T16:20:11.293887Z\", \"is_labeled\": true, \"overlap\": 1, \"inner_id\": 10, \"total_annotations\": 1, \"cancelled_annotations\": 0, \"total_predictions\": 0, \"comment_count\": 0, \"unresolved_comment_count\": 0, \"last_comment_updated_at\": null, \"project\": 1, \"updated_by\": null, \"file_upload\": null, \"comment_authors\": []}, \"result\": [{\"original_length\": 61, \"value\": {\"start\": 1.5313807531380752, \"end\": 4.913179916317992, \"channel\": 0, \"labels\": [\"Speaker A\"]}, \"id\": \"5rUHO\", \"from_name\": \"label\", \"to_name\": \"audio\", \"type\": \"labels\", \"origin\": \"manual\"}, {\"original_length\": 61, \"value\": {\"start\": 10.336820083682008, \"end\": 11.80439330543933, \"channel\": 0, \"labels\": [\"Speaker B\"]}, \"id\": \"C1ziU\", \"from_name\": \"label\", \"to_name\": \"audio\", \"type\": \"labels\", \"origin\": \"manual\"}, {\"original_length\": 61, \"value\": {\"start\": 39.87970711297071, \"end\": 41.85774058577406, \"channel\": 0, \"labels\": [\"Speaker C\"]}, \"id\": \"hD5zJ\", \"from_name\": \"label\", \"to_name\": \"audio\", \"type\": \"labels\", \"origin\": \"manual\"}, {\"original_length\": 61, \"value\": {\"start\": 47.98326359832636, \"end\": 49.70606694560669, \"channel\": 0, \"labels\": [\"Speaker D\"]}, \"id\": \"t-Lct\", \"from_name\": \"label\", \"to_name\": \"audio\", \"type\": \"labels\", \"origin\": \"manual\"}, {\"original_length\": 61, \"value\": {\"start\": 50.40794979079498, \"end\": 53.59832635983263, \"channel\": 0, \"labels\": [\"Speaker E\"]}, \"id\": \"76TVG\", \"from_name\": \"label\", \"to_name\": \"audio\", \"type\": \"labels\", \"origin\": \"manual\"}, {\"original_length\": 61, \"value\": {\"start\": 56.98012552301255, \"end\": 61, \"channel\": 0, \"labels\": [\"Speaker F\"]}, \"id\": \"DzC_L\", \"from_name\": \"label\", \"to_name\": \"audio\", \"type\": \"labels\", \"origin\": \"manual\"}], \"was_cancelled\": false, \"ground_truth\": false, \"created_at\": \"2023-08-04T17:34:56.018687Z\", \"updated_at\": \"2023-08-04T17:34:56.018738Z\", \"draft_created_at\": \"2023-08-04T17:34:07.414372Z\", \"lead_time\": 59.522, \"last_action\": null, \"project\": 1, \"updated_by\": 1, \"parent_prediction\": null, \"parent_annotation\": null, \"last_created_by\": null}\"\"\" # truncated for brevity\n",
    "\n",
    "# Parse the JSON string\n",
    "data = json.loads(data_json)\n",
    "\n",
    "# Initialize the RTTM output string\n",
    "rttm_output = \"\"\n",
    "\n",
    "# Iterate through the results and format them in RTTM\n",
    "for result in data[\"result\"]:\n",
    "    start_time = result[\"value\"][\"start\"]\n",
    "    duration = result[\"value\"][\"end\"] - start_time\n",
    "    speaker = result[\"value\"][\"labels\"][0].split(\" \")[-1] # Extracting the speaker label\n",
    "    channel = result[\"value\"][\"channel\"]\n",
    "    \n",
    "    rttm_line = f\"SPEAKER {data['task']['data']['audio']} 1 {start_time:.2f} {duration:.2f} <NA> <NA> {speaker} <NA> <NA>\\n\"\n",
    "    rttm_output += rttm_line\n",
    "\n",
    "# Print the RTTM output\n",
    "print(rttm_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing broadcastify/output/1...\n",
      "Processing broadcastify/output/10...\n",
      "Processing broadcastify/output/11...\n",
      "Processing broadcastify/output/12...\n",
      "Processing broadcastify/output/13...\n",
      "Processing broadcastify/output/14...\n",
      "Processing broadcastify/output/15...\n",
      "Processing broadcastify/output/16...\n",
      "Processing broadcastify/output/17...\n",
      "Processing broadcastify/output/18...\n",
      "Processing broadcastify/output/19...\n",
      "Processing broadcastify/output/2...\n",
      "Processing broadcastify/output/20...\n",
      "Processing broadcastify/output/21...\n",
      "Processing broadcastify/output/22...\n",
      "Processing broadcastify/output/23...\n",
      "Processing broadcastify/output/24...\n",
      "Processing broadcastify/output/25...\n",
      "Processing broadcastify/output/26...\n",
      "Processing broadcastify/output/3...\n",
      "Processing broadcastify/output/4...\n",
      "Processing broadcastify/output/5...\n",
      "Processing broadcastify/output/6...\n",
      "Processing broadcastify/output/7...\n",
      "Processing broadcastify/output/8...\n",
      "Processing broadcastify/output/9...\n",
      "train.rttm file created successfully!\n",
      "test.rttm file created successfully!\n",
      "development.rttm file created successfully!\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "import json\n",
    "import random\n",
    "\n",
    "def convert_to_rttm(data):\n",
    "    rttm_output = \"\"\n",
    "    for result in data[\"result\"]:\n",
    "        start_time = result[\"value\"][\"start\"]\n",
    "        duration = result[\"value\"][\"end\"] - start_time\n",
    "        speaker = result[\"value\"][\"labels\"][0].split(\" \")[-1]\n",
    "        channel = result[\"value\"][\"channel\"]\n",
    "        rttm_line = f\"SPEAKER {data['task']['data']['audio']} 1 {start_time:.2f} {duration:.2f} <NA> <NA> {speaker} <NA> <NA>\\n\"\n",
    "        rttm_output += rttm_line\n",
    "    return rttm_output\n",
    "\n",
    "\n",
    "bucket_name = 'purple-label-studio-data'\n",
    "prefix = 'broadcastify/output/'\n",
    "\n",
    "# Initialize the GCS client\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "# Initialize the RTTM content for train, test, and development\n",
    "rttm_outputs = {\"train\": \"\", \"test\": \"\", \"development\": \"\"}\n",
    "\n",
    "# Collect blob contents\n",
    "blob_contents = []\n",
    "blobs = bucket.list_blobs(prefix=prefix)\n",
    "for blob in blobs:\n",
    "    if blob.name != prefix:\n",
    "        print(f\"Processing {blob.name}...\")\n",
    "        blob_content = blob.download_as_text()\n",
    "        data = json.loads(blob_content)\n",
    "        rttm_output = convert_to_rttm(data)\n",
    "        blob_contents.append(rttm_output)\n",
    "\n",
    "# Shuffle and split the contents\n",
    "random.shuffle(blob_contents)\n",
    "train_size = int(0.8 * len(blob_contents))\n",
    "test_size = int(0.1 * len(blob_contents))\n",
    "\n",
    "rttm_outputs[\"train\"] = \"\\n\".join(blob_contents[:train_size])\n",
    "rttm_outputs[\"test\"] = \"\\n\".join(blob_contents[train_size:train_size + test_size])\n",
    "rttm_outputs[\"development\"] = \"\\n\".join(blob_contents[train_size + test_size:])\n",
    "\n",
    "# Write the RTTM content to files\n",
    "for split, content in rttm_outputs.items():\n",
    "    with open(f'{split}.rttm', 'w') as file:\n",
    "        file.write(content)\n",
    "        print(f\"{split}.rttm file created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HTTPIterator' object has no attribute 'list_blobs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m blobs\u001b[39m.\u001b[39;49mlist_blobs()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HTTPIterator' object has no attribute 'list_blobs'"
     ]
    }
   ],
   "source": [
    "blobs.list_blobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing broadcastify/output/1...\n",
      "Processing broadcastify/output/10...\n",
      "Processing broadcastify/output/11...\n",
      "Processing broadcastify/output/12...\n",
      "Processing broadcastify/output/13...\n",
      "Processing broadcastify/output/14...\n",
      "Processing broadcastify/output/15...\n",
      "Processing broadcastify/output/16...\n",
      "Processing broadcastify/output/17...\n",
      "Processing broadcastify/output/18...\n",
      "Processing broadcastify/output/19...\n",
      "Processing broadcastify/output/2...\n",
      "Processing broadcastify/output/20...\n",
      "Processing broadcastify/output/21...\n",
      "Processing broadcastify/output/22...\n",
      "Processing broadcastify/output/23...\n",
      "Processing broadcastify/output/24...\n",
      "Processing broadcastify/output/25...\n",
      "Processing broadcastify/output/26...\n",
      "Processing broadcastify/output/3...\n",
      "Processing broadcastify/output/4...\n",
      "Processing broadcastify/output/5...\n",
      "Processing broadcastify/output/6...\n",
      "Processing broadcastify/output/7...\n",
      "Processing broadcastify/output/8...\n",
      "Processing broadcastify/output/9...\n",
      "rttms/train.rttm file created successfully!\n",
      "lsts/train.lst file created successfully!\n",
      "rttms/test.rttm file created successfully!\n",
      "lsts/test.lst file created successfully!\n",
      "rttms/development.rttm file created successfully!\n",
      "lsts/development.lst file created successfully!\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "import json\n",
    "import os\n",
    "\n",
    "def convert_to_rttm(data, filename):\n",
    "    rttm_output = \"\"\n",
    "    for result in data[\"result\"]:\n",
    "        start_time = result[\"value\"][\"start\"]\n",
    "        duration = result[\"value\"][\"end\"] - start_time\n",
    "        speaker = result[\"value\"][\"labels\"][0].split(\" \")[-1]\n",
    "        channel = result[\"value\"][\"channel\"]\n",
    "        rttm_line = f\"SPEAKER {filename} 1 {start_time:.2f} {duration:.2f} <NA> <NA> {speaker} <NA> <NA>\\n\"\n",
    "        rttm_output += rttm_line\n",
    "    return rttm_output\n",
    "\n",
    "def download_audio_file(bucket, audio_path):\n",
    "    audio_directory = 'audio'\n",
    "    if not os.path.exists(audio_directory):\n",
    "        os.mkdir(audio_directory)\n",
    "    \n",
    "    # Remove the gs:// prefix and the bucket name from the path\n",
    "    blob = bucket.blob(audio_path.replace('gs://', '').split('/', 1)[1])\n",
    "    local_path = os.path.join(audio_directory, os.path.basename(audio_path))\n",
    "    blob.download_to_filename(local_path)\n",
    "    return os.path.basename(audio_path)\n",
    "\n",
    "\n",
    "bucket_name = 'purple-label-studio-data'\n",
    "prefix = 'broadcastify/output/'\n",
    "\n",
    "# Initialize the GCS client\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "# Initialize the RTTM content and filenames list for train, test, and development\n",
    "rttm_outputs = {\"train\": \"\", \"test\": \"\", \"development\": \"\"}\n",
    "filenames_lists = {\"train\": [], \"test\": [], \"development\": []}\n",
    "\n",
    "# Collect blob contents\n",
    "blob_contents = []\n",
    "file_names = []\n",
    "blobs = bucket.list_blobs(prefix=prefix)\n",
    "for blob in blobs:\n",
    "    if blob.name != prefix:  # Skip the directory itself\n",
    "        print(f\"Processing {blob.name}...\")\n",
    "        blob_content = blob.download_as_text()\n",
    "        data = json.loads(blob_content)\n",
    "        filename = os.path.basename(data['task']['data']['audio'])\n",
    "        rttm_output = convert_to_rttm(data, filename)\n",
    "        blob_contents.append(rttm_output)\n",
    "\n",
    "        # Download the original audio file\n",
    "        downloaded_filename = download_audio_file(bucket, data['task']['data']['audio'])\n",
    "        file_names.append(downloaded_filename)\n",
    "\n",
    "# Shuffle and split the contents\n",
    "train_size = int(0.8 * len(blob_contents))\n",
    "test_size = int(0.1 * len(blob_contents))\n",
    "\n",
    "rttm_outputs[\"train\"] = \"\".join(blob_contents[:train_size])\n",
    "rttm_outputs[\"test\"] = \"\".join(blob_contents[train_size:train_size + test_size])\n",
    "rttm_outputs[\"development\"] = \"\".join(blob_contents[train_size + test_size:])\n",
    "\n",
    "filenames_lists[\"train\"] = file_names[:train_size]\n",
    "filenames_lists[\"test\"] = file_names[train_size:train_size + test_size]\n",
    "filenames_lists[\"development\"] = file_names[train_size + test_size:]\n",
    "\n",
    "# Create directories for rttms and lsts if they don't exist\n",
    "if not os.path.exists('rttms'):\n",
    "    os.mkdir('rttms')\n",
    "if not os.path.exists('lsts'):\n",
    "    os.mkdir('lsts')\n",
    "\n",
    "# Write the RTTM content and filenames to files\n",
    "for split, content in rttm_outputs.items():\n",
    "    with open(f'rttms/{split}.rttm', 'w') as file:\n",
    "        file.write(content)\n",
    "        print(f\"rttms/{split}.rttm file created successfully!\")\n",
    "\n",
    "    with open(f'lsts/{split}.lst', 'w') as file:\n",
    "        file.write(\"\\n\".join(filenames_lists[split]))\n",
    "        print(f\"lsts/{split}.lst file created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate loader registered with `.rttm` suffix\n",
    "speaker = RTTMLoader('rttms/train.rttm')\n",
    "\n",
    "# entries with {placeholders} serve as path templates\n",
    "transcription_template = 'ctms/{uri}.ctm'\n",
    "\n",
    "for resource in protocol.train():\n",
    "    # unique resource identifier\n",
    "    uri = resource['uri']\n",
    "\n",
    "    # only select parts of `rttms/train.rttm` that are relevant to current resource,\n",
    "    # convert it into a convenient data structure (here pyannote.core.Annotation), \n",
    "    # and assign it to `'speaker'` resource key \n",
    "    resource['speaker'] = speaker[uri]\n",
    "\n",
    "    # replace placeholders in `transcription` path template\n",
    "    ctm = transcription_template.format(uri=uri)\n",
    "\n",
    "    # instantiate loader registered with `.ctm` suffix\n",
    "    transcription = CTMLoader(ctm)\n",
    "\n",
    "    # only select parts of the `ctms/{uri}.ctm` that are relevant to current resource\n",
    "    # (here, most likely the whole file), convert it into a convenient data structure\n",
    "    # (here spacy.tokens.Doc), and assign it to `'transcription'` resource key \n",
    "    resource['transcription'] = transcription[uri]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
